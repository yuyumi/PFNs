{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression with TinyPFN\n",
    "\n",
    "This notebook recreates the ridge regression experiment from the original PFN notebook using our TinyPFN implementation.\n",
    "\n",
    "## What is Ridge Regression?\n",
    "\n",
    "Ridge regression follows this prior:\n",
    "- $f = x^T w$ where $w \\sim \\mathcal{N}(0, b^2 I)$\n",
    "- $y \\sim \\mathcal{N}(f, a^2 I)$\n",
    "\n",
    "We will train TinyPFN to learn this pattern from synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tiny_pfn_real import TinyPFNReal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression Data Generation\n",
    "\n",
    "Let's define our `get_batch` function that generates ridge regression datasets, exactly like the original PFN notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_for_ridge_regression(batch_size=2, seq_len=100, num_features=1, \n",
    "                                   hyperparameters=None, device='cpu', **kwargs):\n",
    "    \"\"\"\n",
    "    Generate ridge regression batches exactly like the original PFN notebook.\n",
    "    \n",
    "    Prior: f = x^T w, y ~ Normal(f, a^2 I)\n",
    "    where w ~ Normal(0, b^2 I)\n",
    "    \"\"\"\n",
    "    if hyperparameters is None:\n",
    "        hyperparameters = {'a': 0.1, 'b': 1.0}\n",
    "    \n",
    "    # Sample weights w ~ Normal(0, b^2 I) for each dataset\n",
    "    ws = torch.distributions.Normal(\n",
    "        torch.zeros(num_features + 1), \n",
    "        hyperparameters['b']\n",
    "    ).sample((batch_size,))\n",
    "    \n",
    "    # Sample inputs x ~ Uniform(0, 1)\n",
    "    xs = torch.rand(batch_size, seq_len, num_features)\n",
    "    \n",
    "    # Add bias term (concatenate with ones)\n",
    "    concatenated_xs = torch.cat([xs, torch.ones(batch_size, seq_len, 1)], 2)\n",
    "    \n",
    "    # Compute y = x^T w + noise\n",
    "    ys = torch.distributions.Normal(\n",
    "        torch.einsum('nmf, nf -> nm', concatenated_xs, ws),\n",
    "        hyperparameters['a']\n",
    "    ).sample()[..., None]\n",
    "    \n",
    "    return {\n",
    "        'x': concatenated_xs.to(device),\n",
    "        'y': ys.to(device), \n",
    "        'target_y': ys.to(device)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Ridge Regression Prior\n",
    "\n",
    "Let's sample some datasets from our ridge regression prior to see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample datasets from the prior\n",
    "batch = get_batch_for_ridge_regression(batch_size=10, seq_len=100, num_features=1)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for dataset_index in range(len(batch['x'])):\n",
    "    plt.scatter(batch['x'][dataset_index, :, 0].numpy(), \n",
    "               batch['y'][dataset_index, :].numpy(), alpha=0.6)\n",
    "\n",
    "plt.title('Ridge Regression Prior: f = x^T w, y ~ Normal(f, a^2)\\nw ~ Normal(0, b^2)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Each color represents a different dataset sampled from the ridge regression prior.\")\n",
    "print(\"Notice how each dataset follows a linear trend with noise.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training TinyPFN on Ridge Regression\n",
    "\n",
    "Now let's train our TinyPFN model on ridge regression data. The key insight is that we generate completely new synthetic data for each training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tiny_pfn_on_ridge_regression(epochs=20, batch_size=16, steps_per_epoch=50):\n",
    "    \"\"\"Train TinyPFN on ridge regression data following the original experiment\"\"\"\n",
    "    print(\"Training TinyPFN on Ridge Regression Data\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create TinyPFN model\n",
    "    model = TinyPFNReal(\n",
    "        num_features=2,  # 1 feature + bias\n",
    "        d_model=64,\n",
    "        n_heads=4,\n",
    "        dropout=0.1,\n",
    "        max_seq_len=100\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = []\n",
    "        \n",
    "        for step in range(steps_per_epoch):\n",
    "            # Generate new synthetic data each step - this is the key PFN innovation\n",
    "            batch = get_batch_for_ridge_regression(\n",
    "                batch_size=batch_size, \n",
    "                seq_len=50,  # Smaller sequences for faster training\n",
    "                num_features=1,\n",
    "                hyperparameters={'a': 0.1, 'b': 1.0}\n",
    "            )\n",
    "            \n",
    "            # Split into train/test\n",
    "            train_len = 25\n",
    "            x_train = batch['x'][:, :train_len, :]\n",
    "            y_train = batch['y'][:, :train_len, :]\n",
    "            x_test = batch['x'][:, train_len:, :]\n",
    "            y_test = batch['y'][:, train_len:, :]\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(x_train, y_train, x_test)\n",
    "            loss = criterion(predictions, y_test)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_losses.append(loss.item())\n",
    "        \n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch:3d}/{epochs} | Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    print(f\"Training complete! Final loss: {losses[-1]:.4f}\")\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trained_model, losses = train_tiny_pfn_on_ridge_regression(\n",
    "    epochs=20, \n",
    "    batch_size=16, \n",
    "    steps_per_epoch=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Progress\n",
    "\n",
    "Let's visualize how the training loss decreased over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses)\n",
    "plt.title('TinyPFN Training Progress on Ridge Regression')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of TinyPFN Performance\n",
    "\n",
    "Now let's test our trained model on some ridge regression examples to see how well it learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample some test datasets\n",
    "batch = get_batch_for_ridge_regression(seq_len=100, batch_size=10)\n",
    "trained_model.eval()\n",
    "\n",
    "# Analyze multiple examples\n",
    "for batch_index in range(3):  # Look at first 3 examples\n",
    "    print(f\"\\nExample {batch_index + 1}:\")\n",
    "    \n",
    "    num_training_points = 4\n",
    "    \n",
    "    train_x = batch['x'][batch_index, :num_training_points]\n",
    "    train_y = batch['y'][batch_index, :num_training_points]\n",
    "    test_x = batch['x'][batch_index]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Add batch dimension as transformer expects that\n",
    "        predictions = trained_model(train_x[None], train_y[None], test_x[None])\n",
    "        predictions = predictions[0].squeeze()\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(train_x[..., 0], train_y.squeeze(), color='blue', \n",
    "               label='Training Data', s=50)\n",
    "    \n",
    "    # Sort test points for plotting\n",
    "    order_test_x = test_x[..., 0].argsort()\n",
    "    plt.plot(test_x[order_test_x, 0], predictions[order_test_x], \n",
    "            color='red', linewidth=2, label='TinyPFN Prediction')\n",
    "    \n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(f'TinyPFN Ridge Regression - Example {batch_index + 1}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate simple metrics\n",
    "    mse = torch.mean((predictions[num_training_points:] - \n",
    "                     batch['target_y'][batch_index, num_training_points:].squeeze())**2)\n",
    "    print(f\"MSE on test points: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Context Learning: Effect of Context Size\n",
    "\n",
    "One of the key features of PFNs is in-context learning. Let's see how the predictions improve as we provide more training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a test dataset\n",
    "batch = get_batch_for_ridge_regression(batch_size=1, seq_len=100, num_features=1)\n",
    "test_x = batch['x'][0]\n",
    "test_y = batch['y'][0]\n",
    "\n",
    "context_sizes = [2, 5, 10, 20]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('Effect of Context Size on TinyPFN Predictions', fontsize=14)\n",
    "\n",
    "for i, context_size in enumerate(context_sizes):\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    \n",
    "    train_x = test_x[:context_size]\n",
    "    train_y = test_y[:context_size]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = trained_model(train_x[None], train_y[None], test_x[None])\n",
    "        predictions = predictions[0].squeeze()\n",
    "    \n",
    "    # Plot\n",
    "    ax.scatter(train_x[:, 0].numpy(), train_y.numpy().ravel(), \n",
    "              c='blue', s=50, label=f'Training Data (n={context_size})')\n",
    "    \n",
    "    order_test_x = test_x[:, 0].argsort()\n",
    "    ax.plot(test_x[order_test_x, 0], predictions[order_test_x], \n",
    "            c='red', linewidth=2, label='TinyPFN Prediction')\n",
    "    \n",
    "    ax.set_title(f'Context Size: {context_size}')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Ridge Regression Experiment Complete!**\n",
    "\n",
    "### Key observations:\n",
    "- TinyPFN learns ridge regression from synthetic data\n",
    "- Model shows in-context learning behavior\n",
    "- More context improves prediction quality\n",
    "- Each training step uses completely new synthetic data\n",
    "\n",
    "### What makes this special:\n",
    "1. **No parameter updates during inference** - the model learns purely from context\n",
    "2. **Infinite synthetic training data** - never runs out of new examples\n",
    "3. **Generalizes to new ridge regression problems** - without any fine-tuning\n",
    "4. **Bayesian-like behavior** - provides uncertainty through the learned distribution\n",
    "\n",
    "This demonstrates the core PFN innovation: learning to learn from context using synthetic data generation during training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 