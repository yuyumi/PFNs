{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Let's train our first TinyPFN to do ridge regression\n",
        "\n",
        "TinyPFNs are all about fitting priors, thus we need a prior for this.\n",
        "\n",
        "We will now describe the prior of ridge regression and we will later on prove this in math *and* by example, we will see that the TinyPFN does in fact approximate ridge regression.\n",
        "\n",
        "### The prior\n",
        "\n",
        "Our prior has two parts. i) What the mapping from x to y looks like, that is\n",
        "$$\n",
        "f = x^Tw\\\\\n",
        "y \\sim Normal(f, a^2I).\n",
        "$$\n",
        "We first map to the function value without noise, then we add normal noise.\n",
        "While $x$ is our input, we have two more undefined symbols. $a$ is the standard deviation of our outputs and is fixed upfront to some value, e.g. $0.1$. $w$ on the other hand controls the form of our function, in one d this could e.g. define whether we go up or down with increasing $x$.\n",
        "\n",
        "As $w$ is so crucial, we define a prior over it, too. That is we say $w \\sim Normal(0, b^2I)$ for some other fixed standard deviation $b$. BTW in Bayesian terms a variable like $w$ that controls what our distribution looks like is generally called latent.\n",
        "\n",
        "The above is the whole prior of ridge regression. We define a distribution over $w$ and then given that $w$ we define a distribution over mappings from $x$ to $y$.\n",
        "\n",
        "### Generating prior samples for TinyPFN training\n",
        "\n",
        "All we want to do is train a neural network on datasets sampled from a prior. To do this, we need to define one more thing, that is: we need to define a distribution over $x$ that we consider. For simplicity let us just consider the uniform distribution between 0 and 1.\n",
        "\n",
        "### Let's generate some samples in 1D\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "num_datasets = 10\n",
        "num_points_in_each_dataset = 100\n",
        "\n",
        "# Let's fix our constants\n",
        "a = 0.1\n",
        "b = 1.0\n",
        "\n",
        "# Now we go through the above up from the bottom, first we define our samples of w\n",
        "ws = torch.distributions.Normal(torch.tensor([0.0]), b).sample((num_datasets,)) # a tensor of shape (num_datasets, 1)\n",
        "\n",
        "# For each constant we generate `num_points_in_each_dataset` many x's and y's\n",
        "xs = torch.rand(num_datasets, num_points_in_each_dataset, 1)\n",
        "ys = torch.distributions.Normal(torch.einsum('nmf, nf -> nm', xs, ws), a).sample()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### We can plot our sample datasets\n",
        "If you change the constants above, you have a lot of impact on this plot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for dataset_index in range(num_datasets):\n",
        "    plt.scatter(xs[dataset_index,:,0].numpy(), ys[dataset_index].numpy())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Damn all curves go through the identity...\n",
        "We can fix that by simply appending a $1$ to each $x$. Let's also generally allow different numbers of features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_from_prior(num_datasets = 10, num_features=1, num_points_in_each_dataset = 100,\n",
        "                      hyperparameters={'a': 0.1, 'b': 1.0}):\n",
        "    ws = torch.distributions.Normal(torch.zeros(num_features+1), hyperparameters['b']).sample((num_datasets,)) # a tensor of shape (num_datasets, num_features+1)\n",
        "\n",
        "    xs = torch.rand(num_datasets, num_points_in_each_dataset, num_features)\n",
        "    ys = torch.distributions.Normal(\n",
        "        torch.einsum('nmf, nf -> nm',\n",
        "                     torch.cat([xs,torch.ones(num_datasets, num_points_in_each_dataset,1)],2),\n",
        "                     ws\n",
        "                    ),\n",
        "        hyperparameters['a']\n",
        "    ).sample()\n",
        "    return xs, ys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xs, ys = sample_from_prior()\n",
        "for dataset_index in range(num_datasets):\n",
        "    plt.scatter(xs[dataset_index,:,0].numpy(), ys[dataset_index].numpy())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Ok cool. What now? Let's make something for the TinyPFN to train on..\n",
        "\n",
        "What a TinyPFN does is, given a training set of $(x,y)$ pairs, it learns a predictor internally and predicts the $y$ for another set of $x$ inputs. It also trains like this. During training we automatically split datasets into train and test before feeding them to the TinyPFN.\n",
        "\n",
        "So, all we need to do is refine our `sample_from_prior` to return datasets s.t. it has the format our training function expects.\n",
        "\n",
        "We generally call these functions `get_batch` functions, as they are called before each training step to get the training batches, and they look something like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# in our convention we name the `num_datasets` -> `batch_size`, and the `num_points_in_each_dataset` -> `seq_len`\n",
        "\n",
        "def get_batch_for_ridge_regression(batch_size=2,seq_len=100,num_features=1,\n",
        "                                   hyperparameters=None, device='cpu', **kwargs):\n",
        "    if hyperparameters is None:\n",
        "        hyperparameters = {'a': 0.1, 'b': 1.0}\n",
        "    ws = torch.distributions.Normal(torch.zeros(num_features+1), hyperparameters['b']).sample((batch_size,))\n",
        "\n",
        "    xs = torch.rand(batch_size, seq_len, num_features)\n",
        "    concatenated_xs = torch.cat([xs,torch.ones(batch_size, seq_len,1)],2)\n",
        "\n",
        "    ys = torch.distributions.Normal(\n",
        "        torch.einsum('nmf, nf -> nm',\n",
        "                     concatenated_xs,\n",
        "                     ws\n",
        "                    ),\n",
        "        hyperparameters['a']\n",
        "    ).sample()[..., None]\n",
        "\n",
        "    # Simple return format for TinyPFN\n",
        "    return {'x': concatenated_xs.to(device), 'y': ys.to(device), 'target_y': ys.to(device)}\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Train Train Train!!!\n",
        "\n",
        "### Let's train a mini TinyPFN for 1D datasets with up to 20 elements ðŸš€\n",
        "\n",
        "This should take around 1 minute.\n",
        "\n",
        "#### What happens during training?\n",
        "During training we sample datasets from our `get_batch` method, we then split them into left and right of the so called `single_eval_pos` (which in turn is sampled uniformly at random). If something is left it is training, on the right it is test. Now our TinyPFN is training to predict the $y$ of the test set, given $x$ and $y$ for the training set, as well as $x$ for the test set.\n",
        "\n",
        "Our training uses standard Adam and a MSE loss. We use a transformer as TinyPFN, where each example in the training or test set is encoded as a token (sometimes also called time step).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tiny_pfn import TinyPFN\n",
        "import torch.nn as nn\n",
        "\n",
        "def train_a_tiny_pfn(get_batch_function, epochs=10, max_dataset_size=20, batch_size=16, steps_per_epoch=100):\n",
        "    \n",
        "    # Create TinyPFN model with confidence intervals\n",
        "    model = TinyPFN(\n",
        "        num_features=2,  # 1 feature + bias\n",
        "        d_model=64,\n",
        "        n_heads=4,\n",
        "        dropout=0.1,\n",
        "        max_seq_len=max_dataset_size,\n",
        "        output_mode='distributional',  # Enable confidence intervals\n",
        "        n_mixture_components=3\n",
        "    )\n",
        "    \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
        "    \n",
        "    print(f\"TinyPFN with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "    \n",
        "    # Training loop\n",
        "    model.train()\n",
        "    losses = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        epoch_losses = []\n",
        "        \n",
        "        for step in range(steps_per_epoch):\n",
        "            # Generate new synthetic data each step - this is the key PFN innovation\n",
        "            batch = get_batch_function(\n",
        "                batch_size=batch_size, \n",
        "                seq_len=max_dataset_size,\n",
        "                num_features=1,\n",
        "                hyperparameters={'a': 0.1, 'b': 1.0}\n",
        "            )\n",
        "            \n",
        "            # Split into train/test (like original PFN)\n",
        "            train_len = torch.randint(2, max_dataset_size-2, (1,)).item()\n",
        "            x_train = batch['x'][:, :train_len, :]\n",
        "            y_train = batch['y'][:, :train_len, :]\n",
        "            x_test = batch['x'][:, train_len:, :]\n",
        "            y_test = batch['y'][:, train_len:, :]\n",
        "            \n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(x_train, y_train, x_test)\n",
        "            \n",
        "            # Custom distributional loss\n",
        "            weights, means, stds = model.get_distribution_params(predictions)\n",
        "            \n",
        "            # Negative log-likelihood for mixture of Gaussians\n",
        "            targets = y_test.unsqueeze(-1)\n",
        "            log_probs = []\n",
        "            for i in range(model.n_mixture_components):\n",
        "                component_dist = torch.distributions.Normal(means[..., i], stds[..., i])\n",
        "                log_prob = component_dist.log_prob(targets.squeeze(-1))\n",
        "                log_probs.append(log_prob)\n",
        "            \n",
        "            log_probs = torch.stack(log_probs, dim=-1)\n",
        "            weighted_log_probs = torch.log(weights) + log_probs\n",
        "            mixture_log_prob = torch.logsumexp(weighted_log_probs, dim=-1)\n",
        "            loss = -mixture_log_prob.mean()\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_losses.append(loss.item())\n",
        "        \n",
        "        avg_loss = np.mean(epoch_losses)\n",
        "        losses.append(avg_loss)\n",
        "        \n",
        "        if epoch % 2 == 0:\n",
        "            print(f\"Epoch {epoch:3d}/{epochs} | Loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    return model, losses\n",
        "\n",
        "trained_model = train_a_tiny_pfn(get_batch_for_ridge_regression, epochs=10)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Analysis of the performance of our trained TinyPFN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's sample some datasets to look at\n",
        "batch = get_batch_for_ridge_regression(seq_len=100, batch_size=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# our model wants the seq dimension first, remember that!\n",
        "\n",
        "batch_index = 0 # change this to see other examples\n",
        "num_training_points = 4\n",
        "\n",
        "train_x = batch['x'][batch_index, :num_training_points]\n",
        "train_y = batch['y'][batch_index, :num_training_points]\n",
        "test_x = batch['x'][batch_index]\n",
        "\n",
        "with torch.no_grad():\n",
        "    # we add our batch dimension, as our transformer always expects that\n",
        "    predictions = trained_model(train_x[None], train_y[None], test_x[None])\n",
        "\n",
        "# the model outputs mixture parameters, we need to extract means and confidence intervals\n",
        "pred_means = trained_model.mean(predictions)[0]\n",
        "pred_confs = trained_model.quantile(predictions, quantiles=[0.1, 0.9])[0]\n",
        "\n",
        "plt.scatter(train_x[...,0],train_y.squeeze())\n",
        "order_test_x = test_x[...,0].argsort()\n",
        "plt.plot(test_x[order_test_x, 0],pred_means[order_test_x], color='green', label='TinyPFN')\n",
        "plt.fill_between(test_x[order_test_x,0], pred_confs[order_test_x, 0], pred_confs[order_test_x, 1], alpha=.1, color='green')\n",
        "\n",
        "import sklearn.linear_model\n",
        "\n",
        "ridge_model = sklearn.linear_model.Ridge(alpha=(a/b)**2)\n",
        "ridge_model.fit(train_x,train_y.squeeze())\n",
        "plt.plot(test_x[order_test_x, 0], ridge_model.predict(test_x[order_test_x]), label='ridge regression')\n",
        "plt.legend()\n",
        "plt.plot();\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### They seem to align kinda well..\n",
        "\n",
        ".. at least for a TinyPFN trained for a few minutes. If you are now interested in the more theoretical analysis why this prior yields ridge regression, please see https://statisticaloddsandends.wordpress.com/2018/12/29/bayesian-interpretation-of-ridge-regression/.\n",
        "\n",
        "What we can also see is that our TinyPFN provides us with confidence bounds, unlike ridge regression. This is actually the case out of the box. We use a mixture of gaussians distribution. We just need to find out what good mixture components are for our prior during training.\n",
        "\n",
        "This demonstrates that TinyPFN can learn the same pattern as ridge regression while providing uncertainty estimates!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
